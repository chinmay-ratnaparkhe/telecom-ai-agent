{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5b0a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced AutoEncoder Training for Complete Dataset\n",
    "# This notebook creates a comprehensive AutoEncoder trained on the entire enhanced dataset \n",
    "# with proper overfitting prevention and progress visualization.\n",
    "\n",
    "# Import Required Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(f\"📊 PyTorch version: {torch.__version__}\")\n",
    "print(f\"🎯 CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"🚀 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"\udda5️ Using CPU for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21165a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Prepare Enhanced Dataset\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "print(\"🔄 Loading real telecom data...\")\n",
    "\n",
    "# Load original data\n",
    "data_file = os.path.join('..', 'data', 'AD_data_10KPI.csv')\n",
    "if not os.path.exists(data_file):\n",
    "    data_file = os.path.join('..', 'AD_data_10KPI.csv')\n",
    "\n",
    "if os.path.exists(data_file):\n",
    "    df_original = pd.read_csv(data_file)\n",
    "    print(f\"📈 Original data shape: {df_original.shape}\")\n",
    "    print(\"✅ Data loaded successfully!\")\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(df_original.head())\n",
    "else:\n",
    "    print(f\"❌ Data file not found at {data_file}\")\n",
    "    print(\"Creating sample data for demonstration...\")\n",
    "    \n",
    "    # Create sample data for demonstration\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    n_sites = 20\n",
    "    \n",
    "    data = {\n",
    "        'Date': pd.date_range('2024-01-01', periods=n_samples, freq='H'),\n",
    "        'Site_ID': np.random.choice([f'Site_{i:03d}' for i in range(1, n_sites+1)], n_samples),\n",
    "        'RSRP': np.random.normal(-85, 10, n_samples),\n",
    "        'SINR': np.random.normal(15, 5, n_samples),\n",
    "        'DL_Throughput': np.random.lognormal(3, 0.5, n_samples),\n",
    "        'UL_Throughput': np.random.lognormal(2, 0.5, n_samples),\n",
    "        'CPU_Utilization': np.random.beta(2, 5, n_samples) * 100,\n",
    "        'Active_Users': np.random.poisson(50, n_samples),\n",
    "        'RTT': np.random.gamma(2, 5, n_samples),\n",
    "        'Packet_Loss': np.random.beta(1, 10, n_samples) * 5,\n",
    "        'Call_Drop_Rate': np.random.beta(1, 20, n_samples) * 2,\n",
    "        'Handover_Success_Rate': np.random.beta(8, 2, n_samples) * 100\n",
    "    }\n",
    "    \n",
    "    df_original = pd.DataFrame(data)\n",
    "    print(f\"📈 Sample data created with shape: {df_original.shape}\")\n",
    "    print(\"✅ Sample data ready for training!\")\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(df_original.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aac28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Feature Engineering\n",
    "print(\"🛠️ Performing feature engineering...\")\n",
    "\n",
    "# Define KPI columns\n",
    "kpi_columns = ['RSRP', 'SINR', 'DL_Throughput', 'UL_Throughput', 'CPU_Utilization', \n",
    "               'Active_Users', 'RTT', 'Packet_Loss', 'Call_Drop_Rate', 'Handover_Success_Rate']\n",
    "\n",
    "# Start with original KPIs\n",
    "df_enhanced = df_original.copy()\n",
    "\n",
    "# Sort by Site_ID and Date for time-series features\n",
    "if 'Date' in df_enhanced.columns:\n",
    "    df_enhanced['Date'] = pd.to_datetime(df_enhanced['Date'])\n",
    "    df_enhanced = df_enhanced.sort_values(['Site_ID', 'Date'])\n",
    "\n",
    "print(f\"📊 Starting with {len(kpi_columns)} original KPIs\")\n",
    "\n",
    "# 1. Rolling statistics (moving averages, std)\n",
    "rolling_windows = [3, 6, 12, 24]  # Hours\n",
    "for window in rolling_windows:\n",
    "    for kpi in kpi_columns:\n",
    "        if kpi in df_enhanced.columns:\n",
    "            # Rolling mean\n",
    "            df_enhanced[f'{kpi}_rolling_mean_{window}h'] = (\n",
    "                df_enhanced.groupby('Site_ID')[kpi]\n",
    "                .rolling(window=window, min_periods=1)\n",
    "                .mean()\n",
    "                .reset_index(level=0, drop=True)\n",
    "            )\n",
    "            \n",
    "            # Rolling std\n",
    "            df_enhanced[f'{kpi}_rolling_std_{window}h'] = (\n",
    "                df_enhanced.groupby('Site_ID')[kpi]\n",
    "                .rolling(window=window, min_periods=1)\n",
    "                .std()\n",
    "                .reset_index(level=0, drop=True)\n",
    "            )\n",
    "\n",
    "print(f\"✅ Added rolling statistics for {len(rolling_windows)} windows\")\n",
    "\n",
    "# 2. Lag features\n",
    "lag_periods = [1, 3, 6]  # Hours\n",
    "for lag in lag_periods:\n",
    "    for kpi in kpi_columns:\n",
    "        if kpi in df_enhanced.columns:\n",
    "            df_enhanced[f'{kpi}_lag_{lag}h'] = (\n",
    "                df_enhanced.groupby('Site_ID')[kpi]\n",
    "                .shift(lag)\n",
    "            )\n",
    "\n",
    "print(f\"✅ Added lag features for {len(lag_periods)} periods\")\n",
    "\n",
    "# 3. Rate of change features\n",
    "for kpi in kpi_columns:\n",
    "    if kpi in df_enhanced.columns:\n",
    "        df_enhanced[f'{kpi}_rate_of_change'] = (\n",
    "            df_enhanced.groupby('Site_ID')[kpi]\n",
    "            .pct_change()\n",
    "        )\n",
    "\n",
    "print(\"✅ Added rate of change features\")\n",
    "\n",
    "# 4. Cross-KPI ratios and interactions\n",
    "ratio_pairs = [\n",
    "    ('DL_Throughput', 'UL_Throughput'),\n",
    "    ('RSRP', 'SINR'),\n",
    "    ('Active_Users', 'CPU_Utilization'),\n",
    "    ('Call_Drop_Rate', 'Handover_Success_Rate')\n",
    "]\n",
    "\n",
    "for kpi1, kpi2 in ratio_pairs:\n",
    "    if kpi1 in df_enhanced.columns and kpi2 in df_enhanced.columns:\n",
    "        # Ratio\n",
    "        df_enhanced[f'{kpi1}_{kpi2}_ratio'] = (\n",
    "            df_enhanced[kpi1] / (df_enhanced[kpi2] + 1e-8)  # Add small epsilon to avoid division by zero\n",
    "        )\n",
    "        \n",
    "        # Product\n",
    "        df_enhanced[f'{kpi1}_{kpi2}_product'] = (\n",
    "            df_enhanced[kpi1] * df_enhanced[kpi2]\n",
    "        )\n",
    "\n",
    "print(f\"✅ Added cross-KPI features for {len(ratio_pairs)} pairs\")\n",
    "\n",
    "# 5. Time-based features\n",
    "if 'Date' in df_enhanced.columns:\n",
    "    df_enhanced['hour'] = df_enhanced['Date'].dt.hour\n",
    "    df_enhanced['day_of_week'] = df_enhanced['Date'].dt.dayofweek\n",
    "    df_enhanced['is_weekend'] = (df_enhanced['day_of_week'] >= 5).astype(int)\n",
    "    df_enhanced['is_peak_hour'] = ((df_enhanced['hour'] >= 8) & (df_enhanced['hour'] <= 18)).astype(int)\n",
    "    \n",
    "    print(\"✅ Added temporal features\")\n",
    "\n",
    "# 6. Statistical aggregations per site\n",
    "site_stats = df_enhanced.groupby('Site_ID')[kpi_columns].agg(['mean', 'std']).reset_index()\n",
    "site_stats.columns = ['Site_ID'] + [f'site_{col[0]}_{col[1]}' for col in site_stats.columns[1:]]\n",
    "\n",
    "df_enhanced = df_enhanced.merge(site_stats, on='Site_ID', how='left')\n",
    "print(\"✅ Added site-level statistical features\")\n",
    "\n",
    "# Remove non-numeric columns for training\n",
    "numeric_columns = df_enhanced.select_dtypes(include=[np.number]).columns\n",
    "df_enhanced_numeric = df_enhanced[numeric_columns].copy()\n",
    "\n",
    "# Fill any NaN values with forward fill, then backward fill, then 0\n",
    "df_enhanced_numeric = df_enhanced_numeric.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "\n",
    "print(f\"\\n📈 Feature engineering complete!\")\n",
    "print(f\"🔍 Original features: {len(kpi_columns)}\")\n",
    "print(f\"✨ Enhanced features: {len(df_enhanced_numeric.columns)}\")\n",
    "print(f\"📊 Feature expansion: {len(kpi_columns)} → {len(df_enhanced_numeric.columns)} features\")\n",
    "print(f\"🎯 Added features: {len(df_enhanced_numeric.columns) - len(kpi_columns)}\")\n",
    "\n",
    "print(f\"\\n📋 Feature categories:\")\n",
    "print(f\"  • Original KPIs: {len(kpi_columns)}\")\n",
    "print(f\"  • Rolling statistics: {len([c for c in df_enhanced_numeric.columns if 'rolling' in c])}\")\n",
    "print(f\"  • Lag features: {len([c for c in df_enhanced_numeric.columns if 'lag' in c])}\")\n",
    "print(f\"  • Rate of change: {len([c for c in df_enhanced_numeric.columns if 'rate_of_change' in c])}\")\n",
    "print(f\"  • Cross-KPI features: {len([c for c in df_enhanced_numeric.columns if ('_ratio' in c or '_product' in c)])}\")\n",
    "print(f\"  • Temporal features: {len([c for c in df_enhanced_numeric.columns if c in ['hour', 'day_of_week', 'is_weekend', 'is_peak_hour']])}\")\n",
    "print(f\"  • Site statistics: {len([c for c in df_enhanced_numeric.columns if 'site_' in c])}\")\n",
    "\n",
    "# Display sample of enhanced data\n",
    "print(f\"\\n🔍 Sample of enhanced data:\")\n",
    "print(df_enhanced_numeric.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6877cc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced AutoEncoder Architecture\n",
    "class EnhancedAutoEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced AutoEncoder with regularization and better architecture\n",
    "    for multi-KPI anomaly detection on the complete enhanced dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, encoding_dims=[64, 32, 16], dropout_rate=0.2):\n",
    "        super(EnhancedAutoEncoder, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.encoding_dims = encoding_dims\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # Build encoder layers dynamically\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for i, dim in enumerate(encoding_dims):\n",
    "            encoder_layers.extend([\n",
    "                nn.Linear(prev_dim, dim),\n",
    "                nn.BatchNorm1d(dim),  # Batch normalization for stability\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)  # Dropout for regularization\n",
    "            ])\n",
    "            prev_dim = dim\n",
    "        \n",
    "        # Remove last dropout\n",
    "        encoder_layers = encoder_layers[:-1]\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # Build decoder layers (reverse of encoder)\n",
    "        decoder_layers = []\n",
    "        reverse_dims = list(reversed(encoding_dims[:-1])) + [input_dim]\n",
    "        prev_dim = encoding_dims[-1]\n",
    "        \n",
    "        for i, dim in enumerate(reverse_dims):\n",
    "            if i == len(reverse_dims) - 1:  # Last layer (output)\n",
    "                decoder_layers.extend([\n",
    "                    nn.Linear(prev_dim, dim),\n",
    "                    nn.Sigmoid()  # Sigmoid for normalized output\n",
    "                ])\n",
    "            else:\n",
    "                decoder_layers.extend([\n",
    "                    nn.Linear(prev_dim, dim),\n",
    "                    nn.BatchNorm1d(dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout_rate)\n",
    "                ])\n",
    "            prev_dim = dim\n",
    "        \n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize weights using Xavier initialization\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through encoder and decoder\"\"\"\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, encoded\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Get encoded representation\"\"\"\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self, encoded):\n",
    "        \"\"\"Decode from encoded representation\"\"\"\n",
    "        return self.decoder(encoded)\n",
    "\n",
    "print(\"🏗️ Enhanced AutoEncoder architecture defined!\")\n",
    "print(\"✨ Features:\")\n",
    "print(\"  • Multi-layer encoder/decoder\")\n",
    "print(\"  • Batch normalization for training stability\")\n",
    "print(\"  • Dropout for regularization\")\n",
    "print(\"  • Xavier weight initialization\")\n",
    "print(\"  • Sigmoid output for normalized reconstruction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2630c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation and Training Utilities\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
    "    \n",
    "    def __init__(self, patience=10, min_delta=1e-6, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            \n",
    "        if self.counter >= self.patience:\n",
    "            if self.restore_best_weights:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def save_checkpoint(self, model):\n",
    "        \"\"\"Save model weights\"\"\"\n",
    "        self.best_weights = model.state_dict().copy()\n",
    "\n",
    "def prepare_data_for_training(df, test_size=0.2, val_size=0.2, batch_size=128, random_state=42):\n",
    "    \"\"\"Prepare data for AutoEncoder training with proper splits\"\"\"\n",
    "    print(\"🔄 Preparing data for training...\")\n",
    "    \n",
    "    # Remove non-numeric columns if any\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    X = df[numeric_cols].values\n",
    "    \n",
    "    print(f\"📊 Using {len(numeric_cols)} numeric features\")\n",
    "    print(f\"🔢 Data shape: {X.shape}\")\n",
    "    \n",
    "    # Handle any remaining NaN values\n",
    "    if np.isnan(X).any():\n",
    "        print(\"⚠️ Found NaN values, filling with column means...\")\n",
    "        X = np.nan_to_num(X, nan=np.nanmean(X, axis=0))\n",
    "    \n",
    "    # Split into train, validation, and test\n",
    "    X_temp, X_test = train_test_split(X, test_size=test_size, random_state=random_state)\n",
    "    val_size_adjusted = val_size / (1 - test_size)\n",
    "    X_train, X_val = train_test_split(X_temp, test_size=val_size_adjusted, random_state=random_state)\n",
    "    \n",
    "    print(f\"✂️ Data splits:\")\n",
    "    print(f\"  • Training: {X_train.shape[0]:,} samples ({X_train.shape[0]/X.shape[0]*100:.1f}%)\")\n",
    "    print(f\"  • Validation: {X_val.shape[0]:,} samples ({X_val.shape[0]/X.shape[0]*100:.1f}%)\")\n",
    "    print(f\"  • Test: {X_test.shape[0]:,} samples ({X_test.shape[0]/X.shape[0]*100:.1f}%)\")\n",
    "    \n",
    "    # Normalize data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "    val_tensor = torch.FloatTensor(X_val_scaled).to(device)\n",
    "    test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(train_tensor, train_tensor)\n",
    "    val_dataset = TensorDataset(val_tensor, val_tensor)\n",
    "    test_dataset = TensorDataset(test_tensor, test_tensor)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f\"🚀 Data moved to device: {device}\")\n",
    "    print(\"✅ Data preparation complete!\")\n",
    "    \n",
    "    return {\n",
    "        'train_loader': train_loader,\n",
    "        'val_loader': val_loader, \n",
    "        'test_loader': test_loader,\n",
    "        'scaler': scaler,\n",
    "        'device': device,\n",
    "        'input_dim': X_train_scaled.shape[1]\n",
    "    }\n",
    "\n",
    "print(\"⏱️ Early stopping mechanism ready!\")\n",
    "print(\"🔧 Data preparation utilities defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2cf5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data for Training\n",
    "data_info = prepare_data_for_training(df_enhanced_numeric, batch_size=256)\n",
    "\n",
    "# Training Function\n",
    "def train_enhanced_autoencoder(data_info, epochs=100, learning_rate=0.001, \n",
    "                             weight_decay=1e-5, patience=15, encoding_dims=[64, 32, 16]):\n",
    "    \"\"\"Train enhanced AutoEncoder with all improvements\"\"\"\n",
    "    device = data_info['device']\n",
    "    input_dim = data_info['input_dim']\n",
    "    \n",
    "    print(f\"🚀 Starting enhanced AutoEncoder training\")\n",
    "    print(f\"📊 Input dimension: {input_dim}\")\n",
    "    print(f\"🏗️ Architecture: {input_dim} → {' → '.join(map(str, encoding_dims))} → {' → '.join(map(str, reversed(encoding_dims)))} → {input_dim}\")\n",
    "    print(f\"⏱️ Max epochs: {epochs}\")\n",
    "    print(f\"🎯 Device: {device}\")\n",
    "    print()\n",
    "    \n",
    "    # Initialize model\n",
    "    model = EnhancedAutoEncoder(\n",
    "        input_dim=input_dim, \n",
    "        encoding_dims=encoding_dims,\n",
    "        dropout_rate=0.2\n",
    "    ).to(device)\n",
    "    \n",
    "    # Loss function and optimizer with weight decay\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(patience=patience, min_delta=1e-6)\n",
    "    \n",
    "    # Training history\n",
    "    history = {'train_loss': [], 'val_loss': [], 'lr': []}\n",
    "    \n",
    "    print(\"🔄 Training started...\")\n",
    "    epoch_pbar = tqdm(range(epochs), desc=\"Training\", unit=\"epoch\")\n",
    "    \n",
    "    for epoch in epoch_pbar:\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_batches = 0\n",
    "        \n",
    "        for batch_x, _ in data_info['train_loader']:\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed, encoded = model(batch_x)\n",
    "            loss = criterion(reconstructed, batch_x)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "        \n",
    "        avg_train_loss = train_loss / train_batches\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, _ in data_info['val_loader']:\n",
    "                reconstructed, _ = model(batch_x)\n",
    "                loss = criterion(reconstructed, batch_x)\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "        \n",
    "        avg_val_loss = val_loss / val_batches\n",
    "        scheduler.step(avg_val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        # Update progress bar\n",
    "        epoch_pbar.set_postfix({\n",
    "            'train_loss': f'{avg_train_loss:.6f}',\n",
    "            'val_loss': f'{avg_val_loss:.6f}',\n",
    "            'lr': f'{current_lr:.2e}'\n",
    "        })\n",
    "        \n",
    "        # Early stopping check\n",
    "        if early_stopping(avg_val_loss, model):\n",
    "            print(f\"\\n⏹️ Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    epoch_pbar.close()\n",
    "    print(\"\\n✅ Training completed!\")\n",
    "    return model, history\n",
    "\n",
    "# Train the model\n",
    "print(\"🎯 Training Enhanced AutoEncoder on Complete Feature Dataset\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model, history = train_enhanced_autoencoder(\n",
    "    data_info, \n",
    "    epochs=100,\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=1e-5,\n",
    "    patience=15,\n",
    "    encoding_dims=[64, 32, 16]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "telecom-kpi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
